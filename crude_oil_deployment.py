# -*- coding: utf-8 -*-
"""Crude_Oil_Deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tKBZtJGWLKM3kupp4ebAAdsbBJGeta5v
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("Crude oil.csv")
df

df.dropna(axis=0, inplace=True)

df_copy = df.copy()
df_copy

df_copy['Date'] = pd.to_datetime(df_copy['Date'])
df_copy['year']=df_copy['Date'].dt.year
df_copy['year']

# Ensuring the 'Date' column is in datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Settting 'Date' as the index
df.set_index('Date', inplace=True)
df

# Extracting the selected column as a pandas Series
series = df["Close/Last"].values.reshape(-1, 1)
series

# Normalizing the data into (-1 to 1) using Min-Max scaling
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
series = scaler.fit_transform(series)
series

# Defining a function to prepare the data for LSTM
def prepare_data(data, time_steps):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data[i:i+time_steps])
        y.append(data[i+time_steps])
    return np.array(X), np.array(y)

# Defining the number of time steps and split the data into training and testing sets
time_steps = 10
X, y = prepare_data(series, time_steps)

train_size = int(0.8 * len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

pip install tensorflow

import tensorflow as tf
from tensorflow import keras

# Building an LSTM model
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

model = Sequential()
model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(time_steps, 1)))
model.add(LSTM(units=50, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

# Training the model
model.fit(X_train, y_train, epochs=50, batch_size=64)

# Making predictions on the test data
y_pred = model.predict(X_test)

# Inversing to transform the predictions and actual values to their original scale
y_pred = scaler.inverse_transform(y_pred)
y_pred
y_test = scaler.inverse_transform(y_test)
y_test

# Calculating the root mean squared error (RMSE) as an evaluation metric
from sklearn.metrics import mean_squared_error
from math import sqrt

rmse = sqrt(mean_squared_error(y_test, y_pred))
print(f'Root Mean Squared Error (RMSE): {rmse}')

def MAPE(org,pred):
    temp = np.abs((pred-org)/org)*100
    return np.mean(temp)

MAPE(y_pred,y_test)

# Plotting the actual vs. predicted oil prices
plt.figure(figsize=(12, 6))
plt.plot(y_test, label='Actual')
plt.plot(y_pred, label='Predicted', linestyle='--')
plt.xlabel('Time')
plt.ylabel('Oil Price')
plt.legend()
plt.title('Actual vs. Predicted Oil Prices')
plt.show()

# saving the model
import pickle
pickle_out = open("lstm_model.pkl", mode = "wb")
pickle.dump(model, pickle_out)
pickle_out.close()

pip install streamlit





import streamlit as st
import pandas as pd
import numpy as np
import pickle
import warnings
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
def load_data():
    data = pd.read_csv("Crude oil.csv")
    data['Date'] = pd.to_datetime(data['Date'])
    return data

# Load the pre-trained Linear Regression model
def load_model(model_path):
    with open(model_path, 'rb') as file:
        model = pickle.load(file)
    return model

# Main function to run the Streamlit app
def main():
    st.title('Crude Oil Price Forecasting')

    # Load the dataset
    data = load_data()

    # Load the pre-trained model
    model_path = 'lstm_model.pkl'  # Replace with the actual path to your .pkl file
    model = load_model(model_path)

    # Sidebar to select date range
    st.sidebar.subheader('Select Date Range for Prediction')
    start_date = st.sidebar.date_input('Start Date', data['Date'].min())
    end_date = st.sidebar.date_input('End Date', data['Date'].max())

    # Convert start_date and end_date to numpy.datetime64
    start_date = np.datetime64(start_date)
    end_date = np.datetime64(end_date)

    # Filter the data based on selected date range
    mask = (data['Date'] >= start_date) & (data['Date'] <= end_date)
    filtered_data = data.loc[mask]

    # Input features
    st.sidebar.subheader('Input Features')
    open_price = st.sidebar.number_input('Open Price', value=filtered_data['Open'].mean())
    high_price = st.sidebar.number_input('High Price', value=filtered_data['High'].mean())
    low_price = st.sidebar.number_input('Low Price', value=filtered_data['Low'].mean())
    volume = st.sidebar.number_input('Volume', value=filtered_data['Volume'].mean())

    # Predict the Close/Last price
    features = np.array([open_price, high_price, low_price, volume]).reshape(-1,1)
    predicted_price = model.predict(features)

    st.write('## Predicted Close/Last Price:')
    #st.write(f'${predicted_price[0]:.2f}')
    st.write(predicted_price)

    # Display the historical data
    st.write('## Historical Data')
    st.write(filtered_data)

if __name__ == '__main__':
    main()